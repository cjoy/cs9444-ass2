{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Epoch:  1, Batch:   32, Loss: 0.693\n",
      "Epoch:  1, Batch:   64, Loss: 0.684\n",
      "Epoch:  1, Batch:   96, Loss: 0.656\n",
      "Epoch:  1, Batch:  128, Loss: 0.646\n",
      "Epoch:  1, Batch:  160, Loss: 0.660\n",
      "Epoch:  1, Batch:  192, Loss: 0.642\n",
      "Epoch:  1, Batch:  224, Loss: 0.613\n",
      "Epoch:  1, Batch:  256, Loss: 0.609\n",
      "Epoch:  1, Batch:  288, Loss: 0.605\n",
      "Epoch:  1, Batch:  320, Loss: 0.663\n",
      "Epoch:  1, Batch:  352, Loss: 0.654\n",
      "Epoch:  1, Batch:  384, Loss: 0.601\n",
      "Epoch:  2, Batch:   32, Loss: 0.609\n",
      "Epoch:  2, Batch:   64, Loss: 0.706\n",
      "Epoch:  2, Batch:   96, Loss: 0.689\n",
      "Epoch:  2, Batch:  128, Loss: 0.687\n",
      "Epoch:  2, Batch:  160, Loss: 0.687\n",
      "Epoch:  2, Batch:  192, Loss: 0.678\n",
      "Epoch:  2, Batch:  224, Loss: 0.657\n",
      "Epoch:  2, Batch:  256, Loss: 0.632\n",
      "Epoch:  2, Batch:  288, Loss: 0.627\n",
      "Epoch:  2, Batch:  320, Loss: 0.681\n",
      "Epoch:  2, Batch:  352, Loss: 0.723\n",
      "Epoch:  2, Batch:  384, Loss: 0.636\n",
      "Epoch:  3, Batch:   32, Loss: 0.635\n",
      "Epoch:  3, Batch:   64, Loss: 0.628\n",
      "Epoch:  3, Batch:   96, Loss: 0.624\n",
      "Epoch:  3, Batch:  128, Loss: 0.677\n",
      "Epoch:  3, Batch:  160, Loss: 0.660\n",
      "Epoch:  3, Batch:  192, Loss: 0.634\n",
      "Epoch:  3, Batch:  224, Loss: 0.630\n",
      "Epoch:  3, Batch:  256, Loss: 0.602\n",
      "Epoch:  3, Batch:  288, Loss: 0.577\n",
      "Epoch:  3, Batch:  320, Loss: 0.622\n",
      "Epoch:  3, Batch:  352, Loss: 0.580\n",
      "Epoch:  3, Batch:  384, Loss: 0.555\n",
      "Epoch:  4, Batch:   32, Loss: 0.545\n",
      "Epoch:  4, Batch:   64, Loss: 0.561\n",
      "Epoch:  4, Batch:   96, Loss: 0.632\n",
      "Epoch:  4, Batch:  128, Loss: 0.690\n",
      "Epoch:  4, Batch:  160, Loss: 0.682\n",
      "Epoch:  4, Batch:  192, Loss: 0.681\n",
      "Epoch:  4, Batch:  224, Loss: 0.694\n",
      "Epoch:  4, Batch:  256, Loss: 0.670\n",
      "Epoch:  4, Batch:  288, Loss: 0.680\n",
      "Epoch:  4, Batch:  320, Loss: 0.664\n",
      "Epoch:  4, Batch:  352, Loss: 0.669\n",
      "Epoch:  4, Batch:  384, Loss: 0.657\n",
      "Epoch:  5, Batch:   32, Loss: 0.641\n",
      "Epoch:  5, Batch:   64, Loss: 0.662\n",
      "Epoch:  5, Batch:   96, Loss: 0.673\n",
      "Epoch:  5, Batch:  128, Loss: 0.661\n",
      "Epoch:  5, Batch:  160, Loss: 0.639\n",
      "Epoch:  5, Batch:  192, Loss: 0.652\n",
      "Epoch:  5, Batch:  224, Loss: 0.662\n",
      "Epoch:  5, Batch:  256, Loss: 0.654\n",
      "Epoch:  5, Batch:  288, Loss: 0.639\n",
      "Epoch:  5, Batch:  320, Loss: 0.627\n",
      "Epoch:  5, Batch:  352, Loss: 0.614\n",
      "Epoch:  5, Batch:  384, Loss: 0.542\n",
      "Epoch:  6, Batch:   32, Loss: 0.542\n",
      "Epoch:  6, Batch:   64, Loss: 0.516\n",
      "Epoch:  6, Batch:   96, Loss: 0.467\n",
      "Epoch:  6, Batch:  128, Loss: 0.468\n",
      "Epoch:  6, Batch:  160, Loss: 0.483\n",
      "Epoch:  6, Batch:  192, Loss: 0.494\n",
      "Epoch:  6, Batch:  224, Loss: 0.461\n",
      "Epoch:  6, Batch:  256, Loss: 0.456\n",
      "Epoch:  6, Batch:  288, Loss: 0.471\n",
      "Epoch:  6, Batch:  320, Loss: 0.440\n",
      "Epoch:  6, Batch:  352, Loss: 0.457\n",
      "Epoch:  6, Batch:  384, Loss: 0.427\n",
      "Epoch:  7, Batch:   32, Loss: 0.447\n",
      "Epoch:  7, Batch:   64, Loss: 0.420\n",
      "Epoch:  7, Batch:   96, Loss: 0.452\n",
      "Epoch:  7, Batch:  128, Loss: 0.450\n",
      "Epoch:  7, Batch:  160, Loss: 0.453\n",
      "Epoch:  7, Batch:  192, Loss: 0.453\n",
      "Epoch:  7, Batch:  224, Loss: 0.411\n",
      "Epoch:  7, Batch:  256, Loss: 0.452\n",
      "Epoch:  7, Batch:  288, Loss: 0.429\n",
      "Epoch:  7, Batch:  320, Loss: 0.425\n",
      "Epoch:  7, Batch:  352, Loss: 0.423\n",
      "Epoch:  7, Batch:  384, Loss: 0.408\n",
      "Epoch:  8, Batch:   32, Loss: 0.427\n",
      "Epoch:  8, Batch:   64, Loss: 0.434\n",
      "Epoch:  8, Batch:   96, Loss: 0.406\n",
      "Epoch:  8, Batch:  128, Loss: 0.414\n",
      "Epoch:  8, Batch:  160, Loss: 0.414\n",
      "Epoch:  8, Batch:  192, Loss: 0.418\n",
      "Epoch:  8, Batch:  224, Loss: 0.383\n",
      "Epoch:  8, Batch:  256, Loss: 0.431\n",
      "Epoch:  8, Batch:  288, Loss: 0.414\n",
      "Epoch:  8, Batch:  320, Loss: 0.419\n",
      "Epoch:  8, Batch:  352, Loss: 0.428\n",
      "Epoch:  8, Batch:  384, Loss: 0.418\n",
      "Epoch:  9, Batch:   32, Loss: 0.404\n",
      "Epoch:  9, Batch:   64, Loss: 0.412\n",
      "Epoch:  9, Batch:   96, Loss: 0.405\n",
      "Epoch:  9, Batch:  128, Loss: 0.391\n",
      "Epoch:  9, Batch:  160, Loss: 0.402\n",
      "Epoch:  9, Batch:  192, Loss: 0.399\n",
      "Epoch:  9, Batch:  224, Loss: 0.403\n",
      "Epoch:  9, Batch:  256, Loss: 0.405\n",
      "Epoch:  9, Batch:  288, Loss: 0.414\n",
      "Epoch:  9, Batch:  320, Loss: 0.399\n",
      "Epoch:  9, Batch:  352, Loss: 0.400\n",
      "Epoch:  9, Batch:  384, Loss: 0.393\n",
      "Epoch: 10, Batch:   32, Loss: 0.397\n",
      "Epoch: 10, Batch:   64, Loss: 0.374\n",
      "Epoch: 10, Batch:   96, Loss: 0.371\n",
      "Epoch: 10, Batch:  128, Loss: 0.374\n",
      "Epoch: 10, Batch:  160, Loss: 0.385\n",
      "Epoch: 10, Batch:  192, Loss: 0.406\n",
      "Epoch: 10, Batch:  224, Loss: 0.383\n",
      "Epoch: 10, Batch:  256, Loss: 0.386\n",
      "Epoch: 10, Batch:  288, Loss: 0.390\n",
      "Epoch: 10, Batch:  320, Loss: 0.397\n",
      "Epoch: 10, Batch:  352, Loss: 0.397\n",
      "Epoch: 10, Batch:  384, Loss: 0.383\n",
      "Classification accuracy: 81.91%\n",
      "Matthews Correlation Coefficient: 0.64\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "part2.py\n",
    "\n",
    "UNSW COMP9444 Neural Networks and Deep Learning\n",
    "\n",
    "ONLY COMPLETE METHODS AND CLASSES MARKED \"TODO\".\n",
    "\n",
    "DO NOT MODIFY IMPORTS. DO NOT ADD EXTRA FUNCTIONS.\n",
    "DO NOT MODIFY EXISTING FUNCTION SIGNATURES.\n",
    "DO NOT IMPORT ADDITIONAL LIBRARIES.\n",
    "DOING SO MAY CAUSE YOUR CODE TO FAIL AUTOMATED TESTING.\n",
    "\n",
    "YOU MAY MODIFY THE LINE net = NetworkLstm().to(device)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as tnn\n",
    "import torch.optim as topti\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "\n",
    "# Class for creating the neural network.\n",
    "class NetworkLstm(tnn.Module):\n",
    "    \"\"\"\n",
    "    Implement an LSTM-based network that accepts batched 50-d\n",
    "    vectorized inputs, with the following structure:\n",
    "    LSTM(hidden dim = 100) -> Linear(64) -> ReLu-> Linear(1)\n",
    "    Assume batch-first ordering.\n",
    "    Output should be 1d tensor of shape [batch_size].\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NetworkLstm, self).__init__()\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        Create and initialise weights and biases for the layers.\n",
    "        \"\"\"\n",
    "        self.lstm = tnn.LSTM(50, 100, batch_first=True)\n",
    "        self.fc1 = tnn.Linear(100, 64)\n",
    "        self.fc2 = tnn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, input, length):\n",
    "        \"\"\"\n",
    "        DO NOT MODIFY FUNCTION SIGNATURE\n",
    "        TODO:\n",
    "        Create the forward pass through the network.\n",
    "        \"\"\"\n",
    "        batchSize, _, _ = input.size()\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        h0 = torch.randn(1, batchSize, 100).to(device)\n",
    "        c0 = torch.randn(1, batchSize, 100).to(device)\n",
    "        out = tnn.utils.rnn.pack_padded_sequence(input, length, batch_first=True)\n",
    "        out, (hn, cn) = self.lstm(out, (h0, c0))\n",
    "        out = tnn.functional.relu(self.fc1(hn))\n",
    "        out = self.fc2(out)\n",
    "        out = out.view(batchSize)\n",
    "        return out\n",
    "\n",
    "# Class for creating the neural network.\n",
    "class NetworkCnn(tnn.Module):\n",
    "    \"\"\"\n",
    "    Implement a Convolutional Neural Network.\n",
    "    All conv layers should be of the form:\n",
    "    conv1d(channels=50, kernel size=8, padding=5)\n",
    "\n",
    "    Conv -> ReLu -> maxpool(size=4) -> Conv -> ReLu -> maxpool(size=4) ->\n",
    "    Conv -> ReLu -> maxpool over time (global pooling) -> Linear(1)\n",
    "\n",
    "    The max pool over time operation refers to taking the\n",
    "    maximum val from the entire output channel. See Kim et. al. 2014:\n",
    "    https://www.aclweb.org/anthology/D14-1181/\n",
    "    Assume batch-first ordering.\n",
    "    Output should be 1d tensor of shape [batch_size].\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NetworkCnn, self).__init__()\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        Create and initialise weights and biases for the layers.\n",
    "        \"\"\"\n",
    "        self.conv1 = tnn.Sequential(\n",
    "            tnn.Conv1d(50, 50, kernel_size = 8, padding = 5),\n",
    "            tnn.ReLU(),\n",
    "            tnn.MaxPool1d(kernel_size=4)\n",
    "        )\n",
    "\n",
    "        self.conv2 = tnn.Sequential(\n",
    "            tnn.Conv1d(50, 50, kernel_size = 8, padding = 5),\n",
    "            tnn.ReLU(),\n",
    "            tnn.MaxPool1d(kernel_size=4)\n",
    "        )            \n",
    "            \n",
    "        self.conv3 = tnn.Sequential(\n",
    "            tnn.Conv1d(50, 50, kernel_size = 8, padding = 5),\n",
    "            tnn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc1 = tnn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, input, length):\n",
    "        \"\"\"\n",
    "        DO NOT MODIFY FUNCTION SIGNATURE\n",
    "        TODO:\n",
    "        Create the forward pass through the network.\n",
    "        \"\"\"\n",
    "        batchSize, _, _ = input.size()\n",
    "        out = input.permute(0, 2, 1)\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = torch.max(self.conv3(out).permute(0, 2, 1), 1)[0]\n",
    "        out = self.fc1(out)\n",
    "        out = out.view(batchSize)\n",
    "        return out\n",
    "\n",
    "def lossFunc():\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "    Return a loss function appropriate for the above networks that\n",
    "    will add a sigmoid to the output and calculate the binary\n",
    "    cross-entropy.\n",
    "    \"\"\"\n",
    "    return tnn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def measures(outputs, labels):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "    Return (in the following order): the number of true positive\n",
    "    classifications, true negatives, false positives and false\n",
    "    negatives from the given batch outputs and provided labels.\n",
    "\n",
    "    outputs and labels are torch tensors.\n",
    "    \"\"\"\n",
    "    # Inspired by:\n",
    "    # https://gist.github.com/the-bass/cae9f3976866776dea17a5049013258d\n",
    "    sigm = tnn.Sigmoid()\n",
    "    vec = sigm(outputs).round()/labels\n",
    "    tp = torch.sum(vec == 1).item()\n",
    "    tn = torch.sum(torch.isnan(vec)).item()\n",
    "    fp = torch.sum(vec == float('inf')).item()\n",
    "    fn = torch.sum(vec == 0).item()\n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Use a GPU if available, as it should be faster.\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device: \" + str(device))\n",
    "\n",
    "    # Load the training dataset, and create a data loader to generate a batch.\n",
    "    textField = data.Field(lower=True, include_lengths=True, batch_first=True)\n",
    "    labelField = data.Field(sequential=False)\n",
    "\n",
    "    from imdb_dataloader import IMDB\n",
    "    train, dev = IMDB.splits(textField, labelField, train=\"train\", validation=\"dev\")\n",
    "\n",
    "    textField.build_vocab(train, dev, vectors=GloVe(name=\"6B\", dim=50))\n",
    "    labelField.build_vocab(train, dev)\n",
    "\n",
    "    trainLoader, testLoader = data.BucketIterator.splits((train, dev), shuffle=True, batch_size=64,\n",
    "                                                         sort_key=lambda x: len(x.text), sort_within_batch=True)\n",
    "\n",
    "    # Create an instance of the network in memory (potentially GPU memory). Can change to NetworkCnn during development.\n",
    "    net = NetworkLstm().to(device)\n",
    "\n",
    "    criterion = lossFunc()\n",
    "    optimiser = topti.Adam(net.parameters(), lr=0.001)  # Minimise the loss using the Adam algorithm.\n",
    "\n",
    "    for epoch in range(10):\n",
    "        running_loss = 0\n",
    "\n",
    "        for i, batch in enumerate(trainLoader):\n",
    "            # Get a batch and potentially send it to GPU memory.\n",
    "            inputs, length, labels = textField.vocab.vectors[batch.text[0]].to(device), batch.text[1].to(\n",
    "                device), batch.label.type(torch.FloatTensor).to(device)\n",
    "\n",
    "            labels -= 1\n",
    "\n",
    "            # PyTorch calculates gradients by accumulating contributions to them (useful for\n",
    "            # RNNs).  Hence we must manually set them to zero before calculating them.\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            # Forward pass through the network.\n",
    "            output = net(inputs, length)\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            # Calculate gradients.\n",
    "            loss.backward()\n",
    "\n",
    "            # Minimise the loss according to the gradient.\n",
    "            optimiser.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 32 == 31:\n",
    "                print(\"Epoch: %2d, Batch: %4d, Loss: %.3f\" % (epoch + 1, i + 1, running_loss / 32))\n",
    "                running_loss = 0\n",
    "\n",
    "    true_pos, true_neg, false_pos, false_neg = 0, 0, 0, 0\n",
    "\n",
    "    # Evaluate network on the test dataset.  We aren't calculating gradients, so disable autograd to speed up\n",
    "    # computations and reduce memory usage.\n",
    "    with torch.no_grad():\n",
    "        for batch in testLoader:\n",
    "            # Get a batch and potentially send it to GPU memory.\n",
    "            inputs, length, labels = textField.vocab.vectors[batch.text[0]].to(device), batch.text[1].to(\n",
    "                device), batch.label.type(torch.FloatTensor).to(device)\n",
    "\n",
    "            labels -= 1\n",
    "\n",
    "            outputs = net(inputs, length)\n",
    "\n",
    "            tp_batch, tn_batch, fp_batch, fn_batch = measures(outputs, labels)\n",
    "            true_pos += tp_batch\n",
    "            true_neg += tn_batch\n",
    "            false_pos += fp_batch\n",
    "            false_neg += fn_batch\n",
    "\n",
    "    accuracy = 100 * (true_pos + true_neg) / len(dev)\n",
    "    matthews = MCC(true_pos, true_neg, false_pos, false_neg)\n",
    "\n",
    "    print(\"Classification accuracy: %.2f%%\\n\"\n",
    "          \"Matthews Correlation Coefficient: %.2f\" % (accuracy, matthews))\n",
    "\n",
    "\n",
    "# Matthews Correlation Coefficient calculation.\n",
    "def MCC(tp, tn, fp, fn):\n",
    "    numerator = tp * tn - fp * fn\n",
    "    denominator = ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5\n",
    "\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        return np.divide(numerator, denominator)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
